apiVersion: v1
kind: ConfigMap
metadata:
  name: llm-config
  namespace: llm
data:
  app.yaml: |
    service:
      name: "llm-extraction-platform"
      version: "0.1.0"
      debug: false
      env: "dev"

    server:
      host: "0.0.0.0"
      port: 8000

    # Database: in-cluster DNS name from postgres Service
    database:
      url: "postgresql+asyncpg://llm:llm@postgres:5432/llm"

    # Redis: in-cluster DNS name from redis Service
    redis:
      enabled: true
      url: "redis://redis:6379/0"

    # Model policy defaults (safe for dev)
    model:
      # keep aligned with config/models.yaml you mount or bake into image
      default_id: "meta-llama/Llama-3.2-1B-Instruct"
      dtype: "float16"
      device: "cpu"
      # your backend uses env vars for this too
      # (kept here for clarity/documentation)
      load_mode: "off"

    limits:
      rate_limit_rpm:
        admin: 0
        default: 120
        free: 60
      quota_auto_reset_days: 30

    cache:
      api_key_cache_ttl_seconds: 10
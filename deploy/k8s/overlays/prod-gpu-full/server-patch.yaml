# deploy/k8s/overlays/prod-gpu-full/server-patch.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: server
  namespace: llm
spec:
  replicas: 1
  template:
    spec:
      containers:
        - name: server
          # GHCR (recommended default). Replace <github-username> and <repo-name>.
          image: ghcr.io/<github-username>/<repo-name>/llm-backend:main
          imagePullPolicy: IfNotPresent

          env:
            - name: ENV
              value: "prod"

            - name: MODEL_LOAD_MODE
              value: "eager"
            - name: REQUIRE_MODEL_READY
              value: "1"

            - name: MODEL_DEVICE
              value: "gpu"

            # Option A: stable runtime path; overlay controls contents
            - name: MODELS_YAML
              value: "/app/models/models.yaml"

            - name: REDIS_ENABLED
              value: "true"

            - name: HF_HOME
              value: "/root/.cache/huggingface"

            # OPTIONAL: keep explicit if you want
            - name: APP_ROOT
              value: "/app"
            - name: APP_CONFIG_PATH
              value: "/app/config/server.yaml"

          resources:
            requests:
              cpu: "1"
              memory: "4Gi"
              nvidia.com/gpu: "1"
            limits:
              cpu: "4"
              memory: "16Gi"
              nvidia.com/gpu: "1"

          volumeMounts:
            - name: llm-models
              mountPath: /app/models
              readOnly: true

            - name: hf-cache
              mountPath: /root/.cache/huggingface

      volumes:
        - name: llm-models
          configMap:
            name: llm-models
            items:
              - key: models.yaml
                path: models.yaml

        - name: hf-cache
          persistentVolumeClaim:
            claimName: hf-cache
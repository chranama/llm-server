[project]
name = "llm_server"
version = "1.0.0"
requires-python = "==3.12.*"
dependencies = [
  # --- API / web ---
  "fastapi>=0.116",
  "uvicorn[standard]>=0.35",

  # --- LLM runtime ---
  # ✅ Ministral-3 support requires Transformers v5 prerelease (per Mistral docs)
  "transformers==5.0.0rc0",
  "accelerate>=1.0.0",
  "mistral-common>=1.8.6",
  "safetensors>=0.4.0",

  # --- config / serialization ---
  "python-dotenv>=1.0.0",
  "pydantic-settings>=2.0",
  "orjson>=3.9.15",

  # --- observability ---
  "prometheus-client>=0.20",

  # --- persistence / migrations ---
  "sqlalchemy>=2.0",
  "greenlet>=3.0",
  "alembic>=1.16.4",
  "asyncpg>=0.29",
  "aiosqlite>=0.20",

  # --- caching / http ---
  "redis>=5.0",
  "httpx>=0.27",

  # --- misc ---
  "typing-extensions>=4.8",
  "jsonschema>=4.21,<5.0",
  "typer>=0.12",
  "llm-contracts @ file:../contracts",
]

[project.scripts]
llm = "llm_server.cli:main"

[project.optional-dependencies]
mps = [
  "torch>=2.4",
  "torchvision>=0.19",
  "torchaudio>=2.4",
]
cpu = [
  "torch>=2.4",
  "torchvision>=0.19",
  "torchaudio>=2.4",
]
cuda121 = [
  "torch>=2.4",
  "torchvision>=0.19",
  "torchaudio>=2.4",
]
test = [
  "pytest>=8",
  "httpx>=0.27",
  "asgi-lifespan>=2.1",
  "pytest-cov>=5",
  "coverage>=7",
]
lint = [
  "ruff>=0.5",
  "black>=24",
]

# ---------- uv configuration ----------

[tool.uv.sources]
torch = [
  { index = "pytorch-cpu",   extra = "mps"     },
  { index = "pytorch-cpu",   extra = "cpu"     },
  { index = "pytorch-cu121", extra = "cuda121" },
]
torchvision = [
  { index = "pytorch-cpu",   extra = "mps"     },
  { index = "pytorch-cpu",   extra = "cpu"     },
  { index = "pytorch-cu121", extra = "cuda121" },
]
torchaudio = [
  { index = "pytorch-cpu",   extra = "mps"     },
  { index = "pytorch-cpu",   extra = "cpu"     },
  { index = "pytorch-cu121", extra = "cuda121" },
]

[tool.uv]
package = true

# ✅ allow installing rc/pre-release packages like transformers==5.0.0rc0
prerelease = "allow"

conflicts = [
  [ { extra = "mps" },     { extra = "cpu" } ],
  [ { extra = "mps" },     { extra = "cuda121" } ],
  [ { extra = "cpu" },     { extra = "cuda121" } ],
]

dev-dependencies = [
  "pytest>=8",
  "asgi-lifespan>=2.1",
]

[[tool.uv.index]]
name = "pytorch-cpu"
url = "https://download.pytorch.org/whl/cpu"
explicit = true

[[tool.uv.index]]
name = "pytorch-cu121"
url = "https://download.pytorch.org/whl/cu121"
explicit = true

[build-system]
requires = ["setuptools>=68", "wheel"]
build-server = "setuptools.build_meta"

# If this pyproject.toml lives at server/pyproject.toml, use:
[tool.setuptools.packages.find]
where = ["src"]
include = ["llm_server*"]
exclude = ["tests*", "scripts*", "data*", "notebooks*"]

[tool.setuptools.package-data]
llm_server = ["schemas/*.json"]
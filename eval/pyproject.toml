[project]
name = "llm_eval"
version = "0.1.0"
description = "Evaluation and research toolkit for llm-server"
requires-python = "==3.12.*"

dependencies = [
  # HTTP + API interaction
  "httpx>=0.27",

  # Dataset handling
  "datasets>=2.21",

  # Schema validation / metrics
  "jsonschema>=4.21,<5.0",

  # Reporting / analytics
  "pandas>=2.2",
  "numpy>=1.26",

  # CLI / config
  "python-dotenv>=1.0.0",
  "pydantic-settings>=2.0",
]

[project.scripts]
eval = "llm_eval.cli:main"

[project.optional-dependencies]
test = [
  "pytest>=8",
  "pytest-asyncio>=0.23",
  "pytest-cov>=5",
  "coverage>=7",
]

lint = [
  "ruff>=0.5",
  "black>=24",
]

viz = [
    "matplotlib>=3.9", 
    "seaborn>=0.13",
]

[build-system]
requires = ["setuptools>=68", "wheel"]
build-backend = "setuptools.build_meta"

[tool.setuptools.packages.find]
where = ["src"]
include = ["llm_eval*"]
exclude = ["tests*"]

[tool.setuptools.package-data]
llm_eval = ["**/*.yaml", "**/*.json"]

[tool.pytest.ini_options]
addopts = "-q --cov=llm_eval --cov-report=term-missing"
testpaths = ["tests"]
asyncio_mode = "auto"
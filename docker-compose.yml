services:
  # ==========================
  # Data layer
  # ==========================
  postgres:
    image: postgres:16-alpine
    container_name: llm_postgres
    restart: unless-stopped
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-llm}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-llm}
      POSTGRES_DB: ${POSTGRES_DB:-llm}
    ports:
      - "5433:5432"
    volumes:
      - pg_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-llm} -d ${POSTGRES_DB:-llm}"]
      interval: 5s
      timeout: 3s
      retries: 10

  redis:
    image: redis:7-alpine
    container_name: llm_redis
    restart: unless-stopped
    command: ["redis-server", "--save", "", "--appendonly", "no"]
    volumes:
      - redis_data:/data
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD-SHELL", "redis-cli ping | grep PONG"]
      interval: 5s
      timeout: 3s
      retries: 10

  # ==========================
  # API + LLM (CPU container mode)
  # ==========================
  api:
    build:
      context: .
      dockerfile: Dockerfile.api
    container_name: llm_api
    restart: unless-stopped

    # Use .env so this deployment is clearly separated
    env_file:
      - .env

    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy

    ports:
      - "8000:8000"

    volumes:
      - ${HOME}/.cache/huggingface:/root/.cache/huggingface

    # API is "ready" only when DB + LLM are ready
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8000/readyz || exit 1"]
      interval: 30s
      timeout: 10s
      start_period: 600s  # allow time for first Llama download
      retries: 10

  # ==========================
  # Tooling / DB admin
  # ==========================
  pgadmin:
    image: dpage/pgadmin4:8
    container_name: llm_pgadmin
    restart: unless-stopped
    environment:
      PGADMIN_DEFAULT_EMAIL: ${PGADMIN_EMAIL:-admin@example.com}
      PGADMIN_DEFAULT_PASSWORD: ${PGADMIN_PASSWORD:-admin}
      PGADMIN_CONFIG_SERVER_MODE: "False"
    depends_on:
      postgres:
        condition: service_healthy
    ports:
      - "5050:80"

  # ==========================
  # Observability
  # ==========================
  prometheus:
    image: prom/prometheus:v2.54.1
    container_name: llm_prometheus
    restart: unless-stopped
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--web.external-url=http://localhost:8080/prometheus"
      - "--web.route-prefix=/prometheus"
    volumes:
      - ./infra/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    depends_on:
      api:
        condition: service_healthy
    ports:
      - "9090:9090"

  grafana:
    image: grafana/grafana:11.1.0
    container_name: llm_grafana
    restart: unless-stopped
    environment:
      GF_SERVER_ROOT_URL: "%(protocol)s://%(domain)s:%(http_port)s/grafana/"
      GF_SERVER_SERVE_FROM_SUB_PATH: "true"
    volumes:
      - grafana_data:/var/lib/grafana
      # Provisioning (datasources + dashboards YAML)
      - ./infra/grafana/provisioning:/etc/grafana/provisioning
      # JSON dashboards (llm-api-overview.json, prometheus-full.json, etc.)
      - ./infra/grafana/dashboards:/var/lib/grafana/dashboards
    depends_on:
      prometheus:
        condition: service_started
    ports:
      - "3000:3000"

  # ==========================
  # Nginx front-end
  # ==========================
  nginx:
    image: nginx:1.27-alpine
    container_name: llm_nginx
    restart: unless-stopped
    ports:
      - "8080:80"
    volumes:
      - ./infra/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./infra/.htpasswd:/etc/nginx/.htpasswd:ro
    depends_on:
      grafana:
        condition: service_started
      prometheus:
        condition: service_started
      pgadmin:
        condition: service_started

volumes:
  pg_data:
  grafana_data:
  redis_data:
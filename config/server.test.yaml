service:
  name: "LLM Server (test)"
  version: "0.1.0"
  debug: false
  env: "test"

server:
  host: "127.0.0.1"
  port: 8000

api:
  cors_allowed_origins:
    - "*"

capabilities:
  generate: true
  extract: true

# âœ… top-level, matches the log label
model_load_mode: "lazy"
require_model_ready: false
token_counting: false

model:
  default_id: "fake"
  allowed_models: []
  models_config_path: "config/models.test.yaml"
  dtype: "float16"
  device: "cpu"

redis:
  enabled: false

http:
  llm_service_url: "http://127.0.0.1:9001"
  client_timeout_seconds: 5

limits:
  rate_limit_rpm:
    admin: 0
    default: 120
    free: 30
  quota_auto_reset_days: 30

cache:
  api_key_cache_ttl_seconds: 1
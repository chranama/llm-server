# config/models.yaml
default_model: mistralai/Ministral-3-14B-Instruct-2512-BF16

defaults:
  backend: local
  load_mode: lazy
  device: auto
  trust_remote_code: false
  quantization: null
  # llm_config.py expects capabilities as a mapping of booleans (not a list)
  capabilities:
    generate: true
    extract: false

models:
  - id: mistralai/Ministral-3-14B-Instruct-2512-BF16
    backend: local
    load_mode: eager
    dtype: bfloat16
    device: auto
    text_only: false
    max_context: 32768
    trust_remote_code: true
    quantization: null
    capabilities:
      generate: true
      extract: true
    notes: "Primary high-quality instruct model"

  - id: Qwen/Qwen2.5-14B-Instruct
    backend: local
    load_mode: lazy
    dtype: bfloat16
    device: auto
    text_only: true
    max_context: 32768
    trust_remote_code: false
    quantization: null
    capabilities:
      generate: true
      extract: false
    notes: "Peer 14B text-only instruct model"

  - id: mistralai/Mistral-7B-Instruct-v0.3
    backend: local
    load_mode: lazy
    dtype: float16
    device: auto
    text_only: true
    max_context: 8192
    trust_remote_code: false
    quantization: null
    capabilities:
      generate: true
      extract: false
    notes: "Lightweight Mistral 7B instruct"

  - id: Qwen/Qwen2.5-7B-Instruct
    backend: local
    load_mode: lazy
    dtype: float16
    device: auto
    text_only: true
    max_context: 8192
    trust_remote_code: false
    quantization: null
    capabilities:
      generate: true
      extract: false
    notes: "Lightweight Qwen 7B instruct"